{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hamcrest import assert_that, close_to, equal_to, has_items, has_length, calling, raises, any_of, all_of, has_property, has_properties, has_entries\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "from deepchecks import ConditionCategory\n",
    "from deepchecks.core.errors import DeepchecksValueError, DeepchecksNotSupportedError\n",
    "from deepchecks.tabular.checks.model_evaluation import PerformanceDisparityReport\n",
    "from tests.base.utils import equal_condition_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olivieratduke/anaconda3/envs/deepchecks/lib/python3.9/site-packages/category_encoders/one_hot.py:145: FutureWarning:\n",
      "\n",
      "iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "\n",
      "/home/olivieratduke/anaconda3/envs/deepchecks/lib/python3.9/site-packages/category_encoders/one_hot.py:145: FutureWarning:\n",
      "\n",
      "iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deepchecks.tabular.datasets.classification import adult\n",
    "from deepchecks.tabular.datasets.regression import avocado\n",
    "\n",
    "train, test = adult.load_data()\n",
    "model = adult.load_fitted_model()\n",
    "\n",
    "adult_split_dataset_and_model = (train, test, model)\n",
    "\n",
    "train_avocade, test_avocado = avocado.load_data()\n",
    "model_avocado = avocado.load_fitted_model()\n",
    "avocado_split_dataset_and_model = (train_avocade, test_avocado, model_avocado)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test scorer types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Arrange\n",
    "_, test, model = adult_split_dataset_and_model\n",
    "scorer_types = [\n",
    "    None,\n",
    "    \"f1\",\n",
    "    ('f1_score', make_scorer(f1_score, average='micro')),\n",
    "    {'f1_score': make_scorer(f1_score, average='micro')},\n",
    "]\n",
    "\n",
    "# Act\n",
    "for scorer in scorer_types:\n",
    "    check = PerformanceDisparityReport(\"sex\", scorer=scorer)\n",
    "    check.run(test, model)\n",
    "\n",
    "# Assert\n",
    "pass # no error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Arrange\n",
    "    tasks = [\n",
    "        (\n",
    "            *adult_split_dataset_and_model, \n",
    "            [\"sex\", \"age\"], \n",
    "            [\"education\", \"capital-gain\"]\n",
    "        ),\n",
    "        (\n",
    "            *avocado_split_dataset_and_model, \n",
    "            [\"type\", \"year\"], \n",
    "            [\"region\", \"Total Bags\"]\n",
    "        ),\n",
    "    ]\n",
    "    def run_task(train, test, model, protected_feat_to_test, control_feat_to_test):\n",
    "        train = train.sample()\n",
    "        test = test.sample()\n",
    "\n",
    "        for feat1 in protected_feat_to_test:\n",
    "            check = PerformanceDisparityReport(protected_feature=feat1)\n",
    "            check.run(test, model)\n",
    "\n",
    "            for feat2 in control_feat_to_test:\n",
    "                check = PerformanceDisparityReport(protected_feature=feat1, control_feature=feat2)\n",
    "                check.run(test, model)\n",
    "\n",
    "    # Act\n",
    "    for task in tasks:\n",
    "        run_task(*task)\n",
    "\n",
    "    # Assert\n",
    "    pass # no error\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test condition pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange\n",
    "_, test, model = adult_split_dataset_and_model\n",
    "check = PerformanceDisparityReport(\"sex\")\n",
    "check.add_condition_bounded_performance_difference(lower_bound=-0.04)\n",
    "check2 = PerformanceDisparityReport(\"sex\")\n",
    "check2.add_condition_bounded_relative_performance_difference(lower_bound=-0.042)\n",
    "\n",
    "# Act\n",
    "result = check.run(test, model)\n",
    "condition_result = result.conditions_results\n",
    "result2 = check2.run(test, model)\n",
    "condition_result2 = result2.conditions_results\n",
    "\n",
    "# Assert\n",
    "assert_that(condition_result, has_items(has_properties(\n",
    "    category=ConditionCategory.PASS,\n",
    "    name=\"Performance differences are bounded between -0.04 and inf.\",\n",
    "    details=\"Found 0 subgroups with performance differences outside of the given bounds.\"\n",
    ")))\n",
    "assert_that(condition_result2, has_items(has_properties(\n",
    "    category=ConditionCategory.PASS,\n",
    "    name=\"Relative performance differences are bounded between -0.042 and inf.\",\n",
    "    details=\"Found 0 subgroups with relative performance differences outside of the given bounds.\"\n",
    ")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test condition fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange\n",
    "_, test, model = adult_split_dataset_and_model\n",
    "check = PerformanceDisparityReport(\"sex\")\n",
    "check.add_condition_bounded_performance_difference(lower_bound=-0.03)\n",
    "check2 = PerformanceDisparityReport(\"sex\")\n",
    "check2.add_condition_bounded_relative_performance_difference(lower_bound=-0.04)\n",
    "\n",
    "# Act\n",
    "result = check.run(test, model)\n",
    "condition_result = result.conditions_results\n",
    "result2 = check2.run(test, model)\n",
    "condition_result2 = result2.conditions_results\n",
    "\n",
    "# Assert\n",
    "assert_that(condition_result, has_items(has_properties(\n",
    "    category=ConditionCategory.FAIL,\n",
    "    name=\"Performance differences are bounded between -0.03 and inf.\",\n",
    "    details=\"Found 1 subgroups with performance differences outside of the given bounds.\"\n",
    ")))\n",
    "assert_that(condition_result2, has_items(has_properties(\n",
    "    category=ConditionCategory.FAIL,\n",
    "    name=\"Relative performance differences are bounded between -0.04 and inf.\",\n",
    "    details=\"Found 1 subgroups with relative performance differences outside of the given bounds.\"\n",
    ")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run value error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange\n",
    "_, test, model = adult_split_dataset_and_model\n",
    "check = PerformanceDisparityReport(protected_feature=\"sex\")\n",
    "check_invalid1 = PerformanceDisparityReport(protected_feature=\"invalid_feature\")\n",
    "check_invalid2 = PerformanceDisparityReport(protected_feature=\"sex\", control_feature=\"invalid_feature\")\n",
    "check_invalid3 = PerformanceDisparityReport(protected_feature=\"sex\", control_feature=\"sex\")\n",
    "\n",
    "# Act & Assert\n",
    "assert_that(\n",
    "    calling(check.run).with_args(\"invalid_data\"), \n",
    "    raises(DeepchecksValueError, r'non-empty instance of Dataset or DataFrame was expected, instead got str')\n",
    ")\n",
    "assert_that(\n",
    "    calling(check.run).with_args(test), \n",
    "    raises(DeepchecksNotSupportedError, r'Check is irrelevant for Datasets without model')\n",
    ")\n",
    "assert_that(\n",
    "    calling(check_invalid1.run).with_args(test, model), \n",
    "    raises(DeepchecksValueError, r'Feature invalid_feature not found in dataset.')\n",
    ")\n",
    "assert_that(\n",
    "    calling(check_invalid2.run).with_args(test, model), \n",
    "    raises(DeepchecksValueError, r'Feature invalid_feature not found in dataset.')\n",
    ")\n",
    "assert_that(\n",
    "    calling(check_invalid3.run).with_args(test, model), \n",
    "    raises(DeepchecksValueError, r'protected_feature sex and control_feature sex are the same.')\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nExpected: an object with length of <2>\n     but: was <(Scatter({\n    'cliponaxis': False,\n    'hovertemplate': [%{y}: %{x} (group size: 5421)<extra></extra>, baseline: %{x}\n                      (group size: 16281)<extra></extra>],\n    'legendgroup': 'Positive difference',\n    'line': {'color': 'limegreen', 'width': 8},\n    'marker': {'color': [white, #222222],\n               'line': {'color': ['limegreen', 'limegreen'], 'width': [2, 2]},\n               'size': 6,\n               'symbol': 0},\n    'mode': 'lines+text+markers',\n    'opacity': 1,\n    'showlegend': False,\n    'x': [0.9177273565762775, 0.8466310423192679],\n    'xaxis': 'x',\n    'y': [ Female,  Female],\n    'yaxis': 'y'\n}), Scatter({\n    'cliponaxis': False,\n    'hovertemplate': [%{y}: %{x} (group size: 10860)<extra></extra>, baseline:\n                      %{x} (group size: 16281)<extra></extra>],\n    'legendgroup': 'Negative difference',\n    'line': {'color': 'orangered', 'width': 8},\n    'marker': {'color': [white, #222222],\n               'line': {'color': ['orangered', 'orangered'], 'width': [2, 2]},\n               'size': 6,\n               'symbol': 0},\n    'mode': 'lines+text+markers',\n    'opacity': 1,\n    'showlegend': False,\n    'x': [0.8111418047882136, 0.8466310423192679],\n    'xaxis': 'x',\n    'y': [ Male,  Male],\n    'yaxis': 'y'\n}), Scatter({\n    'legendgroup': 'Negative difference',\n    'legendgrouptitle': {'text': 'Negative difference'},\n    'marker': {'color': 'white', 'line': {'color': 'orangered', 'width': 2}, 'size': 6, 'symbol': 0},\n    'mode': 'markers',\n    'name': 'subgroup score',\n    'x': [None],\n    'y': [None]\n}), Scatter({\n    'legendgroup': 'Negative difference',\n    'legendgrouptitle': {'text': 'Negative difference'},\n    'marker': {'color': '#222222', 'line': {'color': 'orangered', 'width': 2}, 'size': 6, 'symbol': 0},\n    'mode': 'markers',\n    'name': 'baseline score',\n    'x': [None],\n    'y': [None]\n}), Scatter({\n    'legendgroup': 'Positive difference',\n    'legendgrouptitle': {'text': 'Positive difference'},\n    'marker': {'color': 'white', 'line': {'color': 'limegreen', 'width': 2}, 'size': 6, 'symbol': 0},\n    'mode': 'markers',\n    'name': 'subgroup score',\n    'x': [None],\n    'y': [None]\n}), Scatter({\n    'legendgroup': 'Positive difference',\n    'legendgrouptitle': {'text': 'Positive difference'},\n    'marker': {'color': '#222222', 'line': {'color': 'limegreen', 'width': 2}, 'size': 6, 'symbol': 0},\n    'mode': 'markers',\n    'name': 'baseline score',\n    'x': [None],\n    'y': [None]\n}))> with length of <6>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_920491/1873311458.py\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Assert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0massert_that\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0massert_that\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0massert_that\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_entries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchecks/lib/python3.9/site-packages/hamcrest/core/assert_that.py\u001b[0m in \u001b[0;36massert_that\u001b[0;34m(actual_or_assertion, matcher, reason)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \"\"\"\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0m_assert_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactual_or_assertion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_or_assertion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepchecks/lib/python3.9/site-packages/hamcrest/core/assert_that.py\u001b[0m in \u001b[0;36m_assert_match\u001b[0;34m(actual, matcher, reason)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe_mismatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nExpected: an object with length of <2>\n     but: was <(Scatter({\n    'cliponaxis': False,\n    'hovertemplate': [%{y}: %{x} (group size: 5421)<extra></extra>, baseline: %{x}\n                      (group size: 16281)<extra></extra>],\n    'legendgroup': 'Positive difference',\n    'line': {'color': 'limegreen', 'width': 8},\n    'marker': {'color': [white, #222222],\n               'line': {'color': ['limegreen', 'limegreen'], 'width': [2, 2]},\n               'size': 6,\n               'symbol': 0},\n    'mode': 'lines+text+markers',\n    'opacity': 1,\n    'showlegend': False,\n    'x': [0.9177273565762775, 0.8466310423192679],\n    'xaxis': 'x',\n    'y': [ Female,  Female],\n    'yaxis': 'y'\n}), Scatter({\n    'cliponaxis': False,\n    'hovertemplate': [%{y}: %{x} (group size: 10860)<extra></extra>, baseline:\n                      %{x} (group size: 16281)<extra></extra>],\n    'legendgroup': 'Negative difference',\n    'line': {'color': 'orangered', 'width': 8},\n    'marker': {'color': [white, #222222],\n               'line': {'color': ['orangered', 'orangered'], 'width': [2, 2]},\n               'size': 6,\n               'symbol': 0},\n    'mode': 'lines+text+markers',\n    'opacity': 1,\n    'showlegend': False,\n    'x': [0.8111418047882136, 0.8466310423192679],\n    'xaxis': 'x',\n    'y': [ Male,  Male],\n    'yaxis': 'y'\n}), Scatter({\n    'legendgroup': 'Negative difference',\n    'legendgrouptitle': {'text': 'Negative difference'},\n    'marker': {'color': 'white', 'line': {'color': 'orangered', 'width': 2}, 'size': 6, 'symbol': 0},\n    'mode': 'markers',\n    'name': 'subgroup score',\n    'x': [None],\n    'y': [None]\n}), Scatter({\n    'legendgroup': 'Negative difference',\n    'legendgrouptitle': {'text': 'Negative difference'},\n    'marker': {'color': '#222222', 'line': {'color': 'orangered', 'width': 2}, 'size': 6, 'symbol': 0},\n    'mode': 'markers',\n    'name': 'baseline score',\n    'x': [None],\n    'y': [None]\n}), Scatter({\n    'legendgroup': 'Positive difference',\n    'legendgrouptitle': {'text': 'Positive difference'},\n    'marker': {'color': 'white', 'line': {'color': 'limegreen', 'width': 2}, 'size': 6, 'symbol': 0},\n    'mode': 'markers',\n    'name': 'subgroup score',\n    'x': [None],\n    'y': [None]\n}), Scatter({\n    'legendgroup': 'Positive difference',\n    'legendgrouptitle': {'text': 'Positive difference'},\n    'marker': {'color': '#222222', 'line': {'color': 'limegreen', 'width': 2}, 'size': 6, 'symbol': 0},\n    'mode': 'markers',\n    'name': 'baseline score',\n    'x': [None],\n    'y': [None]\n}))> with length of <6>\n"
     ]
    }
   ],
   "source": [
    "# Arrange\n",
    "_, test, model = adult_split_dataset_and_model\n",
    "check = PerformanceDisparityReport(\"sex\")\n",
    "\n",
    "expected_value = pd.DataFrame({\n",
    "    'sex': {0: ' Male', 1: ' Female'},\n",
    "    '_scorer': {0: 'Accuracy', 1: 'Accuracy'},\n",
    "    '_score': {0: 0.8111418047882136, 1: 0.9177273565762775},\n",
    "    '_baseline': {0: 0.8466310423192679, 1: 0.8466310423192679},\n",
    "    '_baseline_count': {0: 16281, 1: 16281},\n",
    "    '_count': {0: 10860, 1: 5421},\n",
    "    '_diff': {0: -0.03548923753105426, 1: 0.07109631425700957}\n",
    "})\n",
    "\n",
    "# Act\n",
    "result = check.run(test, model)\n",
    "\n",
    "# Assert\n",
    "assert_that(result.display, has_length(1))\n",
    "assert_that(result.display[0].data, has_length(2))\n",
    "assert_that(result.value.round(3).to_dict(), has_entries(expected_value.round(3).to_dict()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test NA scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange\n",
    "_, test, model = adult_split_dataset_and_model\n",
    "check = PerformanceDisparityReport(\"sex\", min_subgroup_size=1_000_000_000)\n",
    "\n",
    "expected_value = pd.DataFrame({\n",
    "    'sex': {0: ' Male', 1: ' Female'},\n",
    "    '_scorer': {0: 'Accuracy', 1: 'Accuracy'},\n",
    "    '_score': {0: np.nan, 1: np.nan},\n",
    "    '_baseline': {0: np.nan, 1: np.nan},\n",
    "    '_baseline_count': {0: 16281, 1: 16281},\n",
    "    '_count': {0: 10860, 1: 5421},\n",
    "    '_diff': {0: np.nan, 1: np.nan}\n",
    "})\n",
    "\n",
    "# Act\n",
    "result = check.run(test, model)\n",
    "\n",
    "# Assert\n",
    "assert_that(result.value[\"_score\"].isna().all())\n",
    "assert_that(result.value[\"_baseline\"].isna().all())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepchecks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "daedff040d2fbce6480319e9fe34f0086a1d245d7f8877797ba25999b9a20843"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
