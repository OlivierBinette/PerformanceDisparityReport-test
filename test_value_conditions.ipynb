{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hamcrest import assert_that, close_to, equal_to, has_items, has_length, calling, raises, any_of, all_of, has_property, has_properties, has_entries\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "from deepchecks import ConditionCategory\n",
    "from deepchecks.core.errors import DeepchecksValueError, DeepchecksNotSupportedError\n",
    "from deepchecks.tabular.checks.model_evaluation import PerformanceDisparityReport\n",
    "from tests.base.utils import equal_condition_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular.datasets.classification import adult\n",
    "from deepchecks.tabular.datasets.regression import avocado\n",
    "\n",
    "train, test = adult.load_data()\n",
    "model = adult.load_fitted_model()\n",
    "\n",
    "adult_split_dataset_and_model = (train, test, model)\n",
    "\n",
    "train_avocade, test_avocado = avocado.load_data()\n",
    "model_avocado = avocado.load_fitted_model()\n",
    "avocado_split_dataset_and_model = (train_avocade, test_avocado, model_avocado)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Arrange\n",
    "    tasks = [\n",
    "        (\n",
    "            *adult_split_dataset_and_model, \n",
    "            [\"sex\", \"age\"], \n",
    "            [\"education\", \"capital-gain\"]\n",
    "        ),\n",
    "        (\n",
    "            *avocado_split_dataset_and_model, \n",
    "            [\"type\", \"year\"], \n",
    "            [\"region\", \"Total Bags\"]\n",
    "        ),\n",
    "    ]\n",
    "    def run_task(train, test, model, protected_feat_to_test, control_feat_to_test):\n",
    "        train = train.sample()\n",
    "        test = test.sample()\n",
    "\n",
    "        for feat1 in protected_feat_to_test:\n",
    "            check = PerformanceDisparityReport(protected_feature=feat1)\n",
    "            check.run(test, model)\n",
    "\n",
    "            for feat2 in control_feat_to_test:\n",
    "                check = PerformanceDisparityReport(protected_feature=feat1, control_feature=feat2)\n",
    "                check.run(test, model)\n",
    "\n",
    "    # Act\n",
    "    for task in tasks:\n",
    "        run_task(*task)\n",
    "\n",
    "    # Assert\n",
    "    pass # no error\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test condition pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange\n",
    "_, test, model = adult_split_dataset_and_model\n",
    "check = PerformanceDisparityReport(\"sex\")\n",
    "check.add_condition_bounded_performance_difference(lower_bound=-0.04)\n",
    "check2 = PerformanceDisparityReport(\"sex\")\n",
    "check2.add_condition_bounded_relative_performance_difference(lower_bound=-0.042)\n",
    "\n",
    "# Act\n",
    "result = check.run(test, model)\n",
    "condition_result = result.conditions_results\n",
    "result2 = check2.run(test, model)\n",
    "condition_result2 = result2.conditions_results\n",
    "\n",
    "# Assert\n",
    "assert_that(condition_result, has_items(has_properties(\n",
    "    category=ConditionCategory.PASS,\n",
    "    name=\"Performance differences are bounded between -0.04 and inf.\",\n",
    "    details=\"Found 0 subgroups with performance differences outside of the given bounds.\"\n",
    ")))\n",
    "assert_that(condition_result2, has_items(has_properties(\n",
    "    category=ConditionCategory.PASS,\n",
    "    name=\"Relative performance differences are bounded between -0.042 and inf.\",\n",
    "    details=\"Found 0 subgroups with relative performance differences outside of the given bounds.\"\n",
    ")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test condition fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange\n",
    "_, test, model = adult_split_dataset_and_model\n",
    "check = PerformanceDisparityReport(\"sex\")\n",
    "check.add_condition_bounded_performance_difference(lower_bound=-0.03)\n",
    "check2 = PerformanceDisparityReport(\"sex\")\n",
    "check2.add_condition_bounded_relative_performance_difference(lower_bound=-0.04)\n",
    "\n",
    "# Act\n",
    "result = check.run(test, model)\n",
    "condition_result = result.conditions_results\n",
    "result2 = check2.run(test, model)\n",
    "condition_result2 = result2.conditions_results\n",
    "\n",
    "# Assert\n",
    "assert_that(condition_result, has_items(has_properties(\n",
    "    category=ConditionCategory.FAIL,\n",
    "    name=\"Performance differences are bounded between -0.03 and inf.\",\n",
    "    details=\"Found 1 subgroups with performance differences outside of the given bounds.\"\n",
    ")))\n",
    "assert_that(condition_result2, has_items(has_properties(\n",
    "    category=ConditionCategory.FAIL,\n",
    "    name=\"Relative performance differences are bounded between -0.04 and inf.\",\n",
    "    details=\"Found 1 subgroups with relative performance differences outside of the given bounds.\"\n",
    ")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run value error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange\n",
    "_, test, model = adult_split_dataset_and_model\n",
    "check = PerformanceDisparityReport(protected_feature=\"sex\")\n",
    "check_invalid1 = PerformanceDisparityReport(protected_feature=\"invalid_feature\")\n",
    "check_invalid2 = PerformanceDisparityReport(protected_feature=\"sex\", control_feature=\"invalid_feature\")\n",
    "check_invalid3 = PerformanceDisparityReport(protected_feature=\"sex\", control_feature=\"sex\")\n",
    "\n",
    "# Act & Assert\n",
    "assert_that(\n",
    "    calling(check.run).with_args(\"invalid_data\"), \n",
    "    raises(DeepchecksValueError, r'non-empty instance of Dataset or DataFrame was expected, instead got str')\n",
    ")\n",
    "assert_that(\n",
    "    calling(check.run).with_args(test), \n",
    "    raises(DeepchecksNotSupportedError, r'Check is irrelevant for Datasets without model')\n",
    ")\n",
    "assert_that(\n",
    "    calling(check_invalid1.run).with_args(test, model), \n",
    "    raises(DeepchecksValueError, r'Feature invalid_feature not found in dataset.')\n",
    ")\n",
    "assert_that(\n",
    "    calling(check_invalid2.run).with_args(test, model), \n",
    "    raises(DeepchecksValueError, r'Feature invalid_feature not found in dataset.')\n",
    ")\n",
    "assert_that(\n",
    "    calling(check_invalid3.run).with_args(test, model), \n",
    "    raises(DeepchecksValueError, r'protected_feature sex and control_feature sex are the same.')\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange\n",
    "_, test, model = adult_split_dataset_and_model\n",
    "check = PerformanceDisparityReport(\"sex\")\n",
    "\n",
    "expected_value = pd.DataFrame({\n",
    "    'sex': {0: ' Male', 1: ' Female'},\n",
    "    '_scorer': {0: 'Accuracy', 1: 'Accuracy'},\n",
    "    '_score': {0: 0.8111418047882136, 1: 0.9177273565762775},\n",
    "    '_baseline': {0: 0.8466310423192679, 1: 0.8466310423192679},\n",
    "    '_baseline_count': {0: 16281, 1: 16281},\n",
    "    '_count': {0: 10860, 1: 5421},\n",
    "    '_diff': {0: -0.03548923753105426, 1: 0.07109631425700957}\n",
    "})\n",
    "\n",
    "# Act\n",
    "result = check.run(test, model)\n",
    "\n",
    "# Assert\n",
    "assert_that(result.display, has_length(1))\n",
    "assert_that(result.display[0].data, has_length(2))\n",
    "assert_that(result.value.round(3).to_dict(), has_entries(expected_value.round(3).to_dict()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test NA scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange\n",
    "_, test, model = adult_split_dataset_and_model\n",
    "check = PerformanceDisparityReport(\"sex\", min_subgroup_size=1_000_000_000)\n",
    "\n",
    "expected_value = pd.DataFrame({\n",
    "    'sex': {0: ' Male', 1: ' Female'},\n",
    "    '_scorer': {0: 'Accuracy', 1: 'Accuracy'},\n",
    "    '_score': {0: np.nan, 1: np.nan},\n",
    "    '_baseline': {0: np.nan, 1: np.nan},\n",
    "    '_baseline_count': {0: 16281, 1: 16281},\n",
    "    '_count': {0: 10860, 1: 5421},\n",
    "    '_diff': {0: np.nan, 1: np.nan}\n",
    "})\n",
    "\n",
    "# Act\n",
    "result = check.run(test, model)\n",
    "\n",
    "# Assert\n",
    "assert_that(result.value[\"_score\"].isna().all())\n",
    "assert_that(result.value[\"_baseline\"].isna().all())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepchecks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "daedff040d2fbce6480319e9fe34f0086a1d245d7f8877797ba25999b9a20843"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
